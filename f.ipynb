{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1cXAjNYhuM3qlTgNWuUz1_iQhJVEDMLNw",
     "timestamp": 1765058470513
    }
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_K4sEIRJxmTM_merged"
   },
   "outputs": [],
   "source": [
    "# Install necessary packages for robust URL parsing/feature extraction (from supplementary notebook)\n",
    "!pip install tld\n",
    "!pip install tldextract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNfdK0aSThCP"
   },
   "source": [
    "# CS549 Final Project: Malicious URL Detection\n",
    "\n",
    "**Group Members:**\n",
    "- Nghi Tran\n",
    "- Maksym Yelisyeyev\n",
    "- Wasay Zaman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eL0t-xaVT3L9"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "Our objective is to develop machine learning models to identify malicious URLs. Our project is separated into the following sections:\n",
    "1. Data Preprocessing\n",
    "2. Model Development:\n",
    "  * Supervised Learning:\n",
    "    * SVM\n",
    "    * Logistic Regression\n",
    "    * XGBoost\n",
    "  * Hyperparameter Tuning\n",
    "3. Model Evaluation\n",
    "  * Performance Metrics\n",
    "  * Comparative Analysis\n",
    "4. Individual Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MofmLzDUjAi"
   },
   "source": [
    "# 1. Data Preprocessing\n",
    "\n",
    "https://www.kaggle.com/datasets/sid321axn/malicious-urls-dataset?resource=download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee4ImhMZniO-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "import math\n",
    "import re # Added for new IP/regex features\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC # Moved here for better organization\n",
    "from xgboost import XGBClassifier # Moved here for better organization\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, auc\n",
    ") # Moved here for better organization\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vI43WOQkmTGj"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 39
    },
    "id": "M6ZtrXhvmSLI",
    "outputId": "a24eb371-20b5-4739-83c6-9587eff1de98"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "df = pd.read_csv(\"malicious_phish.csv\")\n",
    "\n",
    "# **NEW STEP: URL Cleaning/Obfuscation Fix (from supplementary notebook)**\n",
    "# Replace obfuscated dots like [.] with actual dots for accurate parsing\n",
    "df['url'] = df['url'].str.replace('[.]', '.', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kUmbcAGnm6H"
   },
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TRK0-3MnvHc"
   },
   "source": [
    "## Feature Extraction/Encoding:\n",
    "  * Length-based features:\n",
    "    * total URL length\n",
    "    * host length\n",
    "    * path length\n",
    "    * query length\n",
    "    * **NEW: first directory length**\n",
    "    * **NEW: TLD length**\n",
    "  * Count-based features:\n",
    "    * number of subdomains\n",
    "    * number of digits\n",
    "    * number of “special” symbols (e.g., -, _, @, ?, %, =, ...)\n",
    "    * **NEW: number of letters**\n",
    "    * **NEW: number of directory separators (/)**\n",
    "    * **NEW: number of query parameters (=)**\n",
    "  * Ratio features:\n",
    "    * digit-to-length\n",
    "    * symbol-to-length\n",
    "    * uppercase-to-length\n",
    "    * **NEW: letter-to-length**\n",
    "  * Entropy:\n",
    "    * Shannon entropy to measure the randomness of characters\n",
    "  * Categorical/Binary features: converted into numerical form through encoding\n",
    "    * protocol type (HTTP vs. HTTPS)\n",
    "    * top-level domain (e.g., “.com”, “.net”, “.xyz”)\n",
    "    * **NEW: presence of IP address**\n",
    "    * **NEW: use of suspicious file extensions (.exe, .zip)**\n",
    "    * **NEW: use of URL shortening services**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmn3u-V_0f0k"
   },
   "outputs": [],
   "source": [
    "# Helper: for URL parsing\n",
    "def safe_urlparse(url):\n",
    "    # parse URL: handling cases without protocol and bad formats\n",
    "    try:\n",
    "        if not url.startswith(('http://', 'https://', 'ftp://')):\n",
    "            return urlparse('http://' + url)\n",
    "        return urlparse(url)\n",
    "    except ValueError:\n",
    "        # return empty parsed result if parsing fails\n",
    "        return urlparse('')\n",
    "\n",
    "# Helper: for first directory length (from supplementary notebook)\n",
    "def first_directory_length(url_path):\n",
    "    path_parts = url_path.split('/')\n",
    "    first_directory = path_parts[1] if len(path_parts) > 1 else ''\n",
    "    return len(first_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E49F9HBXnwno"
   },
   "outputs": [],
   "source": [
    "# Length based features:\n",
    "\n",
    "## Total URL Length: total_len\n",
    "df['total_len'] = df['url'].str.len()\n",
    "\n",
    "## Host length: host_len (actual domain length)\n",
    "df['host_len'] = df['url'].apply(lambda x: len(safe_urlparse(x).netloc))\n",
    "\n",
    "## Path length: path_len (actual path length)\n",
    "df['path_len'] = df['url'].apply(lambda x: len(safe_urlparse(x).path))\n",
    "\n",
    "## Query length: query_len\n",
    "df['query_len'] = df['url'].apply(lambda x: len(safe_urlparse(x).query))\n",
    "\n",
    "## **NEW: First Directory Length**\n",
    "df['first_directory_length'] = df['url'].apply(lambda x: first_directory_length(safe_urlparse(x).path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNACmBYDn9Cv"
   },
   "outputs": [],
   "source": [
    "# Count-based features:\n",
    "\n",
    "## Number of subdomains\n",
    "df['num_subdomains'] = df['url'].apply(lambda x: max(0, safe_urlparse(x).netloc.split(':')[0].count('.') - 1) if safe_urlparse(x).netloc else 0)\n",
    "\n",
    "## Number of digits\n",
    "df['num_digits'] = df['url'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "\n",
    "## Total special characters\n",
    "special_chars = ['-', '_', '@', '?', '%', '=', '&', '#', '!', '*']\n",
    "df['num_special_chars'] = df['url'].apply(lambda x: sum(x.count(char) for char in special_chars))\n",
    "\n",
    "## **NEW: Number of letters (from supplementary notebook)**\n",
    "df['num_letters'] = df['url'].apply(lambda x: sum(c.isalpha() for c in x))\n",
    "\n",
    "## **NEW: Directory Count (number of slashes in the path)**\n",
    "df['dir_count'] = df['url'].apply(lambda x: safe_urlparse(x).path.count('/'))\n",
    "\n",
    "## **NEW: Query Parameter Count (number of '=' signs in the query)**\n",
    "df['query_param_count'] = df['url'].apply(lambda x: safe_urlparse(x).query.count('=') if safe_urlparse(x).query else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsLCw4xap-sG"
   },
   "outputs": [],
   "source": [
    "# Ratio features:\n",
    "\n",
    "## Digit to length ratio\n",
    "df['digit_ratio'] = df['num_digits'] / df['total_len']\n",
    "\n",
    "## Special character to length ratio\n",
    "df['special_char_ratio'] = df['num_special_chars'] / df['total_len']\n",
    "\n",
    "## Uppercase to length ratio\n",
    "df['num_uppercase'] = df['url'].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "df['uppercase_ratio'] = df['num_uppercase'] / df['total_len']\n",
    "\n",
    "## **NEW: Letter to length ratio**\n",
    "df['letter_ratio'] = df['num_letters'] / df['total_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-bfXRMGoDn4"
   },
   "outputs": [],
   "source": [
    "# Entropy\n",
    "def calculate_entropy(text):\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    counter = Counter(text)\n",
    "    length = len(text)\n",
    "    entropy = -sum((count/length) * math.log2(count/length) for count in counter.values())\n",
    "    return entropy\n",
    "\n",
    "## URL entropy\n",
    "df['url_entropy'] = df['url'].apply(calculate_entropy)\n",
    "\n",
    "## Host entropy (using safe parsing)\n",
    "df['host_entropy'] = df['url'].apply(lambda x: calculate_entropy(safe_urlparse(x).netloc) if safe_urlparse(x).netloc else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e0oM0UNoFY5"
   },
   "outputs": [],
   "source": [
    "# Categorical and Binary features:\n",
    "\n",
    "## Protocol type: 1 for https, 0 for http\n",
    "df['is_https'] = df['url'].apply(lambda x: 1 if x.startswith('https://') else 0)\n",
    "\n",
    "## Top-level domain\n",
    "df['tld'] = df['url'].apply(lambda x: safe_urlparse(x).netloc.split('.')[-1].lower() if safe_urlparse(x).netloc and '.' in safe_urlparse(x).netloc else 'unknown')\n",
    "\n",
    "## One-hot encode common TLDs\n",
    "df['tld_com'] = (df['tld'] == 'com').astype(int)\n",
    "df['tld_net'] = (df['tld'] == 'net').astype(int)\n",
    "df['tld_org'] = (df['tld'] == 'org').astype(int)\n",
    "df['tld_xyz'] = (df['tld'] == 'xyz').astype(int)\n",
    "\n",
    "## Suspicious TLDs\n",
    "suspicious_tlds = ['xyz', 'tk', 'ml', 'ga', 'cf', 'gq']\n",
    "df['tld_suspicious'] = df['tld'].isin(suspicious_tlds).astype(int)\n",
    "\n",
    "## **NEW: TLD Length**\n",
    "df['tld_length'] = df['tld'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLmpnbWrvuEx"
   },
   "outputs": [],
   "source": [
    "# Potential additional protocol features:\n",
    "# does URL have a protocol?\n",
    "df['has_protocol'] = df['url'].apply(lambda x: 1 if x.startswith(('http://', 'https://', 'ftp://')) else 0)\n",
    "\n",
    "# is this a bare domain? (no protocol, no path)\n",
    "df['is_bare_domain'] = df['url'].apply(lambda x: 1 if not x.startswith(('http://', 'https://')) and x.count('/') == 0 else 0)\n",
    "\n",
    "## **NEW: Has IP Address (from supplementary notebook)**\n",
    "def has_ip_address(url_netloc):\n",
    "    # Simple check for IPv4 address format\n",
    "    ip_match = re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url_netloc)\n",
    "    return 1 if ip_match else 0\n",
    "df['has_ip_address'] = df['url'].apply(lambda x: has_ip_address(safe_urlparse(x).netloc))\n",
    "\n",
    "## **NEW: Suspicious Extension (from supplementary notebook)**\n",
    "susp_ext = ['.exe', '.zip', '.rar', '.js', '.vbs', '.bat', '.cmd', '.php'] # Added .php as it's often used maliciously\n",
    "df['suspicious_extension'] = df['url'].apply(lambda x: 1 if any(ext in x.lower() for ext in susp_ext) else 0)\n",
    "\n",
    "## **NEW: Uses Shortening Service (from supplementary notebook)**\n",
    "shorteners = ['bit.ly', 'goo.gl', 'tinyurl.com', 't.co', 'ow.ly', 'is.gd', 'url.cn', 'cutt.ly', 'bityl.co']\n",
    "df['uses_shortening'] = df['url'].apply(lambda x: 1 if any(short in safe_urlparse(x).netloc for short in shorteners) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1CtZ3YUpgse"
   },
   "outputs": [],
   "source": [
    "print(df.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr0kqPHbgVe9"
   },
   "source": [
    "### Handling Missing Values\n",
    "* Removing rows with missing values\n",
    "* Imputation methods: filling the blanks with estimated values. This could be the mean, median, mode, the nearest data from the same columns,\n",
    "* Interpolation methods: estimate missing values based on the values of surrounding data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYY89N6UqWUo"
   },
   "outputs": [],
   "source": [
    "print(\"Missing values per column:\")\n",
    "feature_cols = [col for col in df.columns if col not in ['url', 'type', 'tld']]\n",
    "missing = df[feature_cols].isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "if df[feature_cols].isnull().sum().sum() == 0:\n",
    "    print(\"\\nNo missing values found in features!\")\n",
    "else:\n",
    "    print(f\"\\nTotal missing values: {df[feature_cols].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqGTb_jWquaY"
   },
   "outputs": [],
   "source": [
    "# Check if same URL has multiple different labels\n",
    "duplicate_urls = df[df['url'].duplicated(keep=False)]\n",
    "\n",
    "if len(duplicate_urls) > 0:\n",
    "    # Group by URL and check if they have different types\n",
    "    url_type_check = duplicate_urls.groupby('url')['type'].nunique()\n",
    "\n",
    "    # URLs with multiple different labels\n",
    "    conflicting_urls = url_type_check[url_type_check > 1]\n",
    "\n",
    "    print(f\"Total duplicate URL entries: {len(duplicate_urls)}\")\n",
    "    print(f\"URLs with conflicting labels: {len(conflicting_urls)}\")\n",
    "\n",
    "    if len(conflicting_urls) > 0:\n",
    "        print(\"\\nExamples of URLs with multiple labels:\")\n",
    "        for url in conflicting_urls.index[:5]:  # Show first 5\n",
    "            print(f\"\\nURL: {url}\")\n",
    "            print(df[df['url'] == url][['url', 'type']])\n",
    "    else:\n",
    "        print(\"\\nAll duplicate URLs have the same label - safe to remove duplicates\")\n",
    "else:\n",
    "    print(\"No duplicate URLs found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZVdWr_J4TE9"
   },
   "source": [
    "There are 6 URLs with conflicting labels. These are likely mislabeled in the dataset, so to be safe, we will be removing these 6 duplicated records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEP1_Ylw4dNZ"
   },
   "outputs": [],
   "source": [
    "# URLs with conflicting labels\n",
    "duplicate_urls = df[df['url'].duplicated(keep=False)]\n",
    "url_type_check = duplicate_urls.groupby('url')['type'].nunique()\n",
    "conflicting_urls = url_type_check[url_type_check > 1].index.tolist()\n",
    "\n",
    "print(f\"URLs with conflicting labels: {len(conflicting_urls)}\")\n",
    "print(\"These URLs will be REMOVED due to unreliable labeling:\")\n",
    "for url in conflicting_urls:\n",
    "    labels = df[df['url'] == url]['type'].unique()\n",
    "    print(f\"  {url}: {labels}\")\n",
    "\n",
    "# Remove conflicting URLs: unreliable data\n",
    "print(f\"\\nRemoving {len(df[df['url'].isin(conflicting_urls)])} rows with conflicting labels\")\n",
    "df_clean = df[~df['url'].isin(conflicting_urls)]\n",
    "\n",
    "# Remove remaining duplicates: same URL, same label: reliable\n",
    "print(f\"Removing remaining duplicates (same URL, same label)\")\n",
    "df_clean = df_clean.drop_duplicates(subset=['url'], keep='first').reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nOriginal shape: {df.shape}\")\n",
    "print(f\"After cleaning: {df_clean.shape}\")\n",
    "print(f\"Total removed: {len(df) - len(df_clean)} rows\")\n",
    "\n",
    "df = df_clean\n",
    "\n",
    "print(\"Final class distribution:\")\n",
    "print(df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzQdviuL5GRa"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "df['type'].value_counts().plot(kind='bar', color='steelblue')\n",
    "plt.title('Distribution of URL Types After Cleaning', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('URL Type', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyOamxLx4230"
   },
   "source": [
    "## EDA: Exploratory Data Analysis\n",
    "\n",
    "[TODO]: Visualize the data to better understand what's going on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUpNAKZC27FB"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmmIeGa67DEO"
   },
   "source": [
    "Binary Classification Approach:\n",
    "\n",
    "Benign: not malicious\n",
    "Defacement, Phishing, and malware: malicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghyaxLrY8i6U"
   },
   "outputs": [],
   "source": [
    "df['is_malicious'] = (df['type'] != 'benign').astype(int)\n",
    "\n",
    "print(\"\\nBinary classification distribution:\")\n",
    "print(df['is_malicious'].value_counts())\n",
    "print(\"\\nProportions:\")\n",
    "print(df['is_malicious'].value_counts(normalize=True))\n",
    "\n",
    "# Visualize binary distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "df['is_malicious'].value_counts().plot(kind='bar', color=['green', 'red'])\n",
    "plt.title('Binary Classification: Benign vs Malicious', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Class (0=Benign, 1=Malicious)', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks([0, 1], ['Benign', 'Malicious'], rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kM472oxv9CQi"
   },
   "source": [
    "## Prepare dataset for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAZGFbJ67CKM"
   },
   "outputs": [],
   "source": [
    "# The features list now includes all the newly created features from both notebooks\n",
    "feature_cols = [col for col in df.columns if col not in ['url', 'type', 'tld', 'is_malicious', 'num_uppercase', 'num_letters']]\n",
    "X = df[feature_cols]\n",
    "y = df['is_malicious'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MeTlbQjg9IDw"
   },
   "outputs": [],
   "source": [
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nFeatures used ({len(feature_cols)}):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xMR5R4T9PG6"
   },
   "source": [
    "## Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zNdVYzV9JnT"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "print(\"\\nClass distribution in train set:\")\n",
    "train_dist = pd.Series(y_train).value_counts()\n",
    "print(f\"  Benign (0): {train_dist[0]:,} ({train_dist[0]/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Malicious (1): {train_dist[1]:,} ({train_dist[1]/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "test_dist = pd.Series(y_test).value_counts()\n",
    "print(f\"  Benign (0): {test_dist[0]:,} ({test_dist[0]/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Malicious (1): {test_dist[1]:,} ({test_dist[1]/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pp7acMNq9grk"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBMEPHcO9hd3"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeature scaling complete!\")\n",
    "print(f\"Scaled training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9pLhlbg9l5L"
   },
   "source": [
    "## Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_8qIfMB9kgo"
   },
   "outputs": [],
   "source": [
    "imbalance_ratio = train_dist.max() / train_dist.min()\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkLoW_AA9pHU"
   },
   "outputs": [],
   "source": [
    "if imbalance_ratio > 1.5:\n",
    "    print(\"Undersampling to balance classes\")\n",
    "\n",
    "    train_data = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
    "    train_data['target'] = y_train\n",
    "\n",
    "    benign_data = train_data[train_data['target'] == 0]\n",
    "    malicious_data = train_data[train_data['target'] == 1]\n",
    "\n",
    "    print(f\"Original - Benign: {len(benign_data):,}, Malicious: {len(malicious_data):,}\")\n",
    "\n",
    "    # Undersample majority class (benign) to 1.2x malicious\n",
    "    target_benign_size = int(len(malicious_data) * 1.2)\n",
    "\n",
    "    benign_downsampled = resample(benign_data,\n",
    "                                   replace=False,\n",
    "                                   n_samples=target_benign_size,\n",
    "                                   random_state=42)\n",
    "\n",
    "    # Combine and shuffle\n",
    "    train_balanced = pd.concat([benign_downsampled, malicious_data])\n",
    "    train_balanced = train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Split back\n",
    "    X_train_scaled = train_balanced[feature_cols].values\n",
    "    y_train = train_balanced['target'].values\n",
    "\n",
    "    print(f\"\\nBalanced training set size: {X_train_scaled.shape}\")\n",
    "    print(\"New class distribution:\")\n",
    "    new_dist = pd.Series(y_train).value_counts()\n",
    "    print(f\"  Benign (0): {new_dist[0]:,} ({new_dist[0]/len(y_train)*100:.1f}%)\")\n",
    "    print(f\"  Malicious (1): {new_dist[1]:,} ({new_dist[1]/len(y_train)*100:.1f}%)\")\n",
    "    print(f\"New imbalance ratio: {new_dist.max() / new_dist.min():.2f}\")\n",
    "else:\n",
    "    print(\"\\nClasses are reasonably balanced. No resampling needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atzpBeMT99iT"
   },
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PFtd1It9_Tt"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFlFVDx3-S_J"
   },
   "outputs": [],
   "source": [
    "print(\"Logistic Regression:\")\n",
    "start_time = time.time()\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_train_time = time.time() - start_time\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_pred_lr_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(f\"Training completed in {lr_train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nx9pe2osF10i"
   },
   "outputs": [],
   "source": [
    "# NOTE: The evaluate_model function is defined later in the 'Evaluation' section.\n",
    "lr_results = evaluate_model(y_test, y_pred_lr, y_pred_lr_proba, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvCcW8ZY-W8i"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KRDR1Hs-YJi"
   },
   "outputs": [],
   "source": [
    "print(\"SVM\")\n",
    "start_time = time.time()\n",
    "svm_model = SVC(kernel='rbf', random_state=42, probability=True)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_train_time = time.time() - start_time\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "y_pred_svm_proba = svm_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(f\"Training completed in {svm_train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a734b2db_merged"
   },
   "outputs": [],
   "source": [
    "svm_results = evaluate_model(y_test, y_pred_svm, y_pred_svm_proba, \"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHd7o1WU-gsu"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMz3OvW_-iHv"
   },
   "outputs": [],
   "source": [
    "print(\"XGBoost\")\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_train_time = time.time() - start_time\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "y_pred_xgb_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(f\"Training completed in {xgb_train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da58f3c2_merged"
   },
   "outputs": [],
   "source": [
    "xgb_results = evaluate_model(y_test, y_pred_xgb, y_pred_xgb_proba, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR1OcPsC-lqu"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwLMUoWJ-niH"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"  Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "    print(f\"  Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                     Predicted\")\n",
    "    print(f\"                Benign    Malicious\")\n",
    "    print(f\"Actual Benign     {cm[0][0]:7,}   {cm[0][1]:7,}\")\n",
    "    print(f\"       Malicious  {cm[1][0]:7,}   {cm[1][1]:7,}\")\n",
    "\n",
    "    # ROC-AUC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a35ba68a"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nComparative Model Evaluation Results\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Collect results in a dictionary for easy DataFrame creation\n",
    "all_results = {\n",
    "    \"Logistic Regression\": lr_results,\n",
    "    \"SVM\": svm_results,\n",
    "    \"XGBoost\": xgb_results\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1-Score\": [],\n",
    "    \"ROC-AUC\": [],\
    "    \"Training Time (s)\": []\n",
    "})\n",
    "\n",
    "for model_name, results in all_results.items():\n",
    "    # Add training time from respective variables\n",
    "    if model_name == \"Logistic Regression\":\n",
    "        train_time = lr_train_time\n",
    "    elif model_name == \"SVM\":\n",
    "        train_time = svm_train_time\n",
    "    elif model_name == \"XGBoost\":\n",
    "        train_time = xgb_train_time\n",
    "    else:\n",
    "        train_time = None\n",
    "\n",
    "    metrics_df.loc[len(metrics_df)] = [\n",
    "        model_name,\n",
    "        results['accuracy'],\n",
    "        results['precision'],\n",
    "        results['recall'],\n",
    "        results['f1'],\n",
    "        results['roc_auc'],\n",
    "        f\"{train_time:.2f}\" if train_time is not None else \"N/A\"\n",
    "    ]\n",
    "\n",
    "metrics_df.set_index('Model', inplace=True)\n",
    "\n",
    "display(metrics_df.round(4))\n",
    "\n",
    "# Optional: Plot ROC curves for comparison\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "\n",
    "for model_name, results in all_results.items():\n",
    "    plt.plot(results['fpr'], results['tpr'], label=f'{model_name} (AUC = {results[\"roc_auc\"]:.4f})')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.03)\n",
    "plt.show()"
   ]
  }
 ]
}
